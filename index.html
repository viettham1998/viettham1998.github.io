<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Viet-Tham Huynh </title> <meta name="author" content="Viet-Tham Huynh"> <meta name="description" content="Viet-Tham Huynh Homepage. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/fav.ico?89ee8eb0f36c50bef024e1ca8be2ff7c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://viettham1998.github.io/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">Home <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Viet-Tham</span> Huynh </h1> <p class="desc"></p> <p><a href="https://en.hcmus.edu.vn/" rel="external nofollow noopener" target="_blank">Researcher at University of Science - VNUHCM, VietNam</a></p> <p><a href="https://www.nii.ac.jp/en/" rel="external nofollow noopener" target="_blank">Student Internship at National Institute of Informatics (NII), Japan</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/my_avt-480.webp 480w,/assets/img/my_avt-800.webp 800w,/assets/img/my_avt-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/my_avt.jpg?4f410bb9e922d2db81ea12840f7dfd6a" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="my_avt.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>227 Nguyen Van Cu</p> <p>Ward 4, District 5</p> <p>Ho Chi Minh City</p> </div> </div> <div class="clearfix"> <p>I am currently a Researcher at the University of Science - VNUHCM, Vietnam. In this role, I focus on advancing cutting-edge technologies and contributing to various innovative projects in the field of computer science.</p> <p>Previously, I completed my undergraduate and master’s programs at the University of Science - VNUHCM under the supervision of <a href="https://scholar.google.com/citations?user=lt2ATkkAAAAJ" rel="external nofollow noopener" target="_blank">Prof. Minh-Triet Tran</a> and <a href="https://scholar.google.com/citations?user=qIaGn7YAAAAJ" rel="external nofollow noopener" target="_blank">Prof. Tam Nguyen</a>. Their mentorship was invaluable in shaping my academic journey and deepening my understanding of complex scientific concepts.</p> <p>My primary research focus is on Augmented Reality (AR) and Virtual Reality (VR). Additionally, I am passionate about Artificial Intelligence (AI) and actively explore its potential applications in AR and VR to enhance user experiences. My goal is to leverage AI to create more immersive and interactive virtual environments.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jun 17, 2024</th> <td> I attended the <strong><a href="https://hvtham.com/assets/img/master_graduation/DSC_2785.jpg" rel="external nofollow noopener" target="_blank">Master’s Graduation Ceremony</a></strong> and received a certificate of merit for <strong><a href="https://hvtham.com/assets/img/master_graduation/DSC_2580.jpg" rel="external nofollow noopener" target="_blank">“Excellence in Learning and Scientific Research”</a></strong> from the President. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 13, 2024</th> <td> I started my internship at <strong><a href="https://www.nii.ac.jp/en/" rel="external nofollow noopener" target="_blank">NII</a></strong> </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 27, 2023</th> <td> I successfully defended my master’s thesis: <strong><a href="#">“Visual understanding to assist 3D scene generation with user interaction”</a></strong> </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 09, 2023</th> <td> I was accepted to intern at <strong><a href="https://www.nii.ac.jp/en/" rel="external nofollow noopener" target="_blank">NII</a></strong> in 2024 </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 07, 2023</th> <td> Attending <strong><a href="#">SOICT 2023</a></strong> in Ho Chi Minh City, Viet Nam. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/psivt23-480.webp 480w,/assets/img/publication_preview/psivt23-800.webp 800w,/assets/img/publication_preview/psivt23-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/psivt23.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="psivt23.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="huynh2023mobilenet" class="col-sm-8"> <div class="title">MobileNet-SA: Lightweight CNN with Self Attention for Sketch Classification</div> <div class="author"> Viet-Tham Huynh , Trong-Thuan Nguyen , Tam V Nguyen , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Minh-Triet Tran' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Pacific-Rim Symposium on Image and Video Technology</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-981-97-0376-0_9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Sketch classification plays a crucial role across diverse domains, including image retrieval, artistic style analysis, and content-based image retrieval. While CNNs have demonstrated remarkable success in various image-related tasks, the computational complexity of large models poses challenges in resource-constrained environments. To address this concern, we propose MobileNet-SA, a novel lightweight model that seamlessly integrates a self-attention module into the MobileNet architecture, with a specific focus on enhancing sketch classification performance. The MobileNet-SA model leverages the inherent efficiency of lightweight CNN while harnessing the power of self-attention mechanisms to effectively capture spatial dependencies and enrich feature representations within sketch data. In our experiments, MobileNet-SA achieves state-of-the-art results, demonstrating an impressive accuracy of 93.5% on the challenging SketchyCOCO dataset and 96.7% on the GM-Sketch dataset. We thoroughly evaluate the model’s performance across diverse sketch classes, confirming its robustness and generalization capabilities, which make it well-suited for real-world applications where input sketches may exhibit significant variations. Our research indicates that MobileNet-SA not only outperforms existing methods but also offers an efficient and interpretable solution for sketch classification tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huynh2023mobilenet</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MobileNet-SA: Lightweight CNN with Self Attention for Sketch Classification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Viet-Tham and Nguyen, Trong-Thuan and Nguyen, Tam V and Tran, Minh-Triet}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Pacific-Rim Symposium on Image and Video Technology}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{110--123}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmm24-480.webp 480w,/assets/img/publication_preview/mmm24-800.webp 800w,/assets/img/publication_preview/mmm24-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mmm24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmm24.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="huynh2024lumos" class="col-sm-8"> <div class="title">LUMOS-DM: Landscape-Based Multimodal Scene Retrieval Enhanced by Diffusion Model</div> <div class="author"> Viet-Tham Huynh , Trong-Thuan Nguyen , Quang-Thuc Nguyen , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Mai-Khiem Tran, Tam V Nguyen, Minh-Triet Tran' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In International Conference on Multimedia Modeling</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-53302-0_11" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Information retrieval is vital in our daily lives, with applications ranging from job searches to academic research. In today’s data-driven world, efficient and accurate retrieval systems are crucial. Our research focuses on video data, using a system called LUMOS-DM: Landscape-based Multimodal Scene Retrieval Enhanced by Diffusion Model. This system leverages Vision Transformer and Diffusion Models, taking user-generated sketch images and text queries as input to generate images for video retrieval. Initial testing on a dataset of 100 h of global landscape videos achieved an 18.78% at Top-20 accuracy rate and 36.45% at Top-100 accuracy rate. Additionally, video retrieval has various applications, including generating data for advertising and marketing. We use a multi-modal approach, combining sketch and text descriptions to enhance video content retrieval, catering to a wide range of user needs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huynh2024lumos</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LUMOS-DM: Landscape-Based Multimodal Scene Retrieval Enhanced by Diffusion Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Viet-Tham and Nguyen, Trong-Thuan and Nguyen, Quang-Thuc and Tran, Mai-Khiem and Nguyen, Tam V and Tran, Minh-Triet}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Multimedia Modeling}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{145--158}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ismar22_02-480.webp 480w,/assets/img/publication_preview/ismar22_02-800.webp 800w,/assets/img/publication_preview/ismar22_02-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/ismar22_02.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ismar22_02.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="nguyen2022data" class="col-sm-8"> <div class="title">Data-Driven City Traffic Planning Simulation</div> <div class="author"> Tam V Nguyen , Thanh Ngoc-Dat Tran , Viet-Tham Huynh , and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Bao Truong, Minh-Quan Le, Mohit Kumavat, Vatsa S Patel, Mai-Khiem Tran, Minh-Triet Tran' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>In 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9974523" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Big cities are well-known for their traffic congestion and high density of vehicles such as cars, buses, trucks, and even a swarm of motorbikes that overwhelm city streets. Large-scale development projects have exacerbated urban conditions, making traffic congestion more severe. In this paper, we proposed a data-driven city traffic planning simulator. In particular, we make use of the city camera system for traffic analysis. It seeks to recognize the traffic vehicles and traffic flows, with reduced intervention from monitoring staff. Then, we develop a city traffic planning simulator upon the analyzed traffic data. The simulator is used to support metropolitan transportation planning. Our experimental findings address traffic planning challenges and the innovative technical solutions needed to solve them in big cities.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nguyen2022data</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Data-Driven City Traffic Planning Simulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen, Tam V and Tran, Thanh Ngoc-Dat and Huynh, Viet-Tham and Truong, Bao and Le, Minh-Quan and Kumavat, Mohit and Patel, Vatsa S and Tran, Mai-Khiem and Tran, Minh-Triet}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{859--864}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mapr23_01-480.webp 480w,/assets/img/publication_preview/mapr23_01-800.webp 800w,/assets/img/publication_preview/mapr23_01-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mapr23_01.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mapr23_01.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="huynh2023light" class="col-sm-8"> <div class="title">Light-weight Sketch Recognition with Knowledge Distillation</div> <div class="author"> Viet-Tham Huynh , Tam V Nguyen , and Minh-Triet Tran </div> <div class="periodical"> <em>In 2023 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10289002" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Recognizing hand-drawn sketches is a promising starting point for various applications, such as assisting artists in creating 3D environments for games or virtual environment scenes quickly and efficiently from concept arts. In addition, by understanding drawings, we can generate 3D models that can be used for further design and development. Thus, in this paper, we aim to develop a novel lightweight network that can accurately recognize sketch drawings. We propose a lightweight-yet-efficient neural network based on MobileNetV2 for sketch recognition and employ knowledge distillation to train the proposed model from EfficientNet-B4. To evaluate the accuracy of the proposed method, we collect a dataset of sketches comprising 1800 drawings in 12 categories, ranging from furniture to animals. The experimental results show that our network model achieves an accuracy of 96.7%, with 96.9% precision, 96.7% recall, and 96.7% F1-score. These results demonstrate that the proposed approach has great potential for practical sketch recognition applications, such as interior design or VR scene generation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huynh2023light</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Light-weight Sketch Recognition with Knowledge Distillation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Viet-Tham and Nguyen, Tam V and Tran, Minh-Triet}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--6}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%68%76%74%68%61%6D@%73%65%6C%61%62.%68%63%6D%75%73.%65%64%75.%76%6E" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-8537-1331" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=mrF9kUEAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.scopus.com/authid/detail.uri?authorId=58059146300" title="Scopus" rel="external nofollow noopener" target="_blank"><i class="ai ai-scopus"></i></a> <a href="https://github.com/viettham1998" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://dblp.org/pid/335/8998" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> </div> <div class="contact-note">You can even add a little note about which of these is the best way to reach you. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Viet-Tham Huynh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Last updated: July 23, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>